# Tracking the Coronavirus on Twitter in 2020

In this project, I scanned through all geotagged tweets sent in 2020 to monitor for the spread of the coronavirus on social media. This project allowed me to:

- work with large scale datasets (~3.3 terabytes)
- work with multilingual text
- utilize the MapReduce divide-and-conquer paradigm to create parallel code
- better understand global perceptions of the coronavirus in the early stages on the pandemic

## Background

Of the approximately 500 million tweets sent everyday, only 2% are *geotagged*. Geotagged tweets contain location information about where the tweets were sent from.
The dataset I used for this project contains all geotagged tweets sent in the year 2020 -- about 1.1 billion tweets in total.

The tweets for each day in 2020 are stored in zip files with the following name format: `geoTwitterYY-MM-DD.zip`. Inside each of these zip files are 24 text files, one for each hour of the day, that contain a single tweet per line in JSON format.

## Project Motivation

Let's say we wanted to simply count the number of tweets sent on a particular day. We could use the following command:

```
unzip -p '/data/Twitter dataset/geoTwitter20-01-01.zip' | wc -l
```

However, the file is so large that looping over it and counting the number of lines takes a very long time -- about 80 seconds on average for me. So, how long would it take for us to loop over the entire dataset?

```
$ echo "print(1887*80/60/60)" | python3
41.93333333333333
```

Based on this, we can predict that it will take about 42 hours just to loop over the entire dataset! Because looping has a runtime of $O(n)$, an algorithm with $O(n^2)$ runtime would take about 73 days to complete. Therefore, it's very important that any algorithms we perform on this dataset have runtime $O(n)$. 73 days is just way too long to sit around waiting for code to finish!

However, we can't let this runtime issue prevent us from digging into this dataset and searching for insights about the spread of coronavirus in 2020. Usually, datasets that contain key insights are quite large. So, we need better ways of dealing with large scale data... introducing: MapReduce.

## MapReduce

MapReduce is a widely used 3-step procedure for large scale parallel processing. The general flow of the MapReduce procedure is as follows:

1. **Partition**: we partition the data into subsets.
2. **Map**: we apply a mapping to the subsets that performs some determined operation.
3. **Reduce**: we merge the outputs generated by the mappings, allowing us to have a single output file by which we can perform analysis on. 

The following image helps to visualize the MapReduce process:

<img src=mapreduce.png width=100% />

The main benefit of MapReduce is that we can run the computationally expensive mapping tasks in parallel, significantly reducing runtime. 

Now that we've introduced the MapReduce model, let's cover each of the steps in a bit more detail. Note that the partition step was completed in the creation of the dataset, as the tweets are already split into one file per day.

## Method

**Mapping**

The `map.py` file processes the zip files for each day and tracks the usage of the loaded hashtags on both a language and country level. The hashtags we track are contained within the `# load keywords` section of the file, and displayed below:

```
hashtags = [
    '#코로나바이러스',   # Korean for Coronavirus
    '#コロナウイルス',   # Japanese for Coronavirus
    '#冠状病毒',         # Chinese for Coronavirus
    '#covid2019',
    '#covid-2019',
    '#covid19',
    '#covid-19',
    '#coronavirus',
    '#corona',
    '#virus',
    '#flu',
    '#sick',
    '#cough',
    '#sneeze',
    '#hospital',
    '#nurse',
    '#doctor',
]
```

Running `map.py` will output two files, one that ends in `.lang` for the language dictionary, and one that ends in `.country` for the country dictionary. Essentially, each file either contains the number of tweets sent from each country (in the `.country` file), or the number of tweets written in each language (in the `.lang` file). The keys of the dictionary in each file are simply the hashtags that we want to track, and the value of those keys are the number of sent tweets that contain the respective hashtag.

**Running the Mapper**

`run_maps.sh` is a shell script that loops over each file in the dataset and runs the `map.py` command on that file. Each call to `map.py` can take up to a day to finish, so I used the `nohup` command to ensure that the program continues to run after disconnecting, and the `&` operator to ensure that all `map.py` commands run in parallel. First, ensure that you have execute permissions:

```
chmod u+x run_maps.sh
```

Then, go ahead and run the script:

```
nohup ./run_maps.sh &
```

**Reducing the Outputs**

After `map.py` has run on all the files, a large number of files will be contained in the `outputs` folder. `reduce.py` combines all of the `.lang` files into a single file, `reduced.lang`, and all of the `.country` files into a different file, `reduced.country`. We make two calls to `reduce.py`, one for the `.lang` files and the other for the `.country` files. Here's an example command to run `reduce.py` for the `.lang` files, using the glob operator:

```
./src/reduce.py --input_path=outputs/*.lang --output_path=reduced.lang
```

A similar command should be used to input all the `.country` files into `reduce.py` to generate `reduced.country`.

**Visualizing the Outputs**

The `visualize.py` file generates a bar graph of the results and stores the bar graph as a png file. The horizontal axis of the graph contains the keys of the input file, and the vertical axis of the graph contains the values of the input file.

First, I ran `visualize.py` with the `--input_path` equal to the country file created in the reduce phase, and the `--key` set to `#coronavirus`. This gave me an idea of which countries sent the most tweets that in some way mentioned the coronavirus. Of course, the results are slightly skewed due to differences in population between the countries, but the result is still somewhat interesting.

<img src=reduced.country_%23coronavirus_graph.png width=100% />

**Task 4: Alternative Reduce**

Create a new file `alternative_reduce.py`.
This file should take as input on the command line a list of hashtags,
and output a line plot where:
1. There is one line per input hashtag.
1. The x-axis is the day of the year.
1. The y-axis is the number of tweets that use that hashtag during the year.

Your `alternative_reduce.py` file have to follow a similar structure to a combined version of the `reduce.py` and `visualize.py` files.
First, you will scan through all of the data in the `outputs` folder created by the mapping step.
In this scan, you will construct a dataset that contains the information that you need to plot.
Then, after you have extracted this information,
you should call the appropriate matplotlib functions to plot the data.

> **HINT:**
> The specifications for this program and plot are intentionally underspecified
> (similar to how many real-world problems are underspecified).
> Feel free to ask clarifying questions.

**Task 5: Uploading**

Commit all of your code and images output files to your github repo and push the results to github.
You must:
1. Delete the current contents of the `README.md` file
1. Insert into the `README.md` file a brief explanation of your project, including the 4 generated png files.
    This explanation should be suitable for a future employer to look at while they are interviewing you to get a rough idea of what you accomplished.
    (And you should tell them about this in your interviews!)

## Submission

Upload a link to you github repository on sakai.
I will look at your code and visualization to determine your grade.

**Grading:**

The assignment is worth 32 points:

1. 8 points for getting the map/reduce to work
1. 8 points for your repo/readme file
1. 8 points for Task 3 plots
1. 8 points for Task 4 plots

The most common ways to miss points are:
1. having incorrect data plotted (because the map program didn't finish running on all of the inputs)
1. having illegible plots that are not "reasonably" formatted

Notice that we are not using CI to grade this assignment.
There's two reasons:

1. You can get slightly different numbers depending on some of the design choices you make in your code.
    For example, should the term `corona` count tweets that contain `coronavirus` as well as tweets that contain just `corona`?
    These are relatively insignificant decisions.
    I'm more concerned with your ability to write a shell script and use `nohup`, `&`, and other process control tools effectively.

1. The dataset is too large to upload to github actions.
    In general, writing test cases for large data analysis tasks is tricky and rarely done.
    Writing correct code without test cases is hard,
    and so many (most?) analysis of large datasets contain lots of bugs.
